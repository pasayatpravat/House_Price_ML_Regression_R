---
title: "1st Assignment - Machine Learning II"
output: 
  html_document:
    toc: true
    toc_depth: 3
author: Pravat Ranjan Pasayat
---
# User details

**User Name**: pasayatpravat

**Display Name**: Pravat

**Best Score**: 0.12017

**Ranking**: 996 as of 16th-Feb-2019)

# Importing Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plyr)
library(dplyr)     # To compute the `union` of the levels.
library(png)       # To include images in this document.
library(knitr)     # To include images inline in this doc.
library(moments)   # Skewness
library(e1071)     # Alternative for Skewness
library(glmnet)    # Lasso
library(caret)     # To enable Lasso training with CV.
library(lubridate) # To use the Date function
```

# Introduction

This assignment focuses on applying the Feature Engineering processes and the Evaluation methods that we have learned in previous sessions to solve a practical scenario: Predict the price of houses.
In particular, we are going to use the experimental scenario proposed by the House Prices Dataset. This dataset includes 79 explanatory variables of residential homes. For more details on the dataset and the competition see <https://www.kaggle.com/c/house-prices-advanced-regression-techniques>.

This dataset is close to the kind of data that you will find in the real world: it is not clean, it might include repeated, correlated or uninformative features, it has null or wrong values... 
Even though this is still far from the kind of messy database that you will find in a real company, it is a good starting point to realize the magnitude of the problem. Therefore, the first step is to visualize and analyze the dataset in order to understand the information that we have. Then, you have to clean the dataset to solve the problems it might present.

Once we have the dataset cleaned, we can start the feature engineering process itself to select the most representative feature set to feed the regression models. Previously to this step, you can create new features or modify the ones already in the dataset. This step typically involves some knowledge domain about the particular scenario of the problem, either because you are an expert on the field or because you have access to people with this knowledge (i.e., the project owner, the business expert in your company,...). Although, I imagine you are not a real-estate expert, there are some sensible procedures or general domain knowledge that you can apply. Moreover, the competition provides a file (`data_description.txt`) that provides an explanation of each of the features that you may find useful for the cleaning and feature engineering process (i.e., this will be the business expert you could have at your company, explaining the data and the related aspects to you). Finally, you also have a lot of kernels at your disposal in the competition webpage to take ideas from. Be creative!

## What is my goal?

- I want to predict the final price of each home (Therefore, this is a regression task).
- I have to clean the dataset to allow its further processing.
- I have to use the feature engineering techniques explained in class to transform the dataset: filtering, wrapper and embedded methods.
- I have to properly apply the evaluation methods and ideas (train, validation, test splitting; cross-validation, chose the proper metric, ..) to understand the real performance of the proposed models, making sure that they will generalize to unseen data (test set).

# Useful Functions

In order to facilitate the evaluation of the impact of the different steps, I am going to place the code for creating a baseline `glm` model in a function. Now I can call it again and again without having to re-write everything. The only thing that changes from one case to another is the dataset that is used to train the model.

1. Function to split a dataset into training and validation.
2. Function to find the mode value of a categorical variable.
3. Function to calculate the number of year differences.
4. Function to find numeric features of a dataset.
5. Function to create polynomial features.
6. Function to standardize the numeric features.

```{r}
# This function splits the dataset into training and validation sets
splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
  index <- 1:nrow(dataframe)
  trainindex <- sample(index, trunc(length(index)/1.5))
  trainset <- dataframe[trainindex, ]
  testset <- dataframe[-trainindex, ]
  list(trainset=trainset,testset=testset)
}

# This function finds the mode of a categorical variable
find_mode <- function(x) {
  x <- as.factor(x);
  ret <- names(sort(table(x), decreasing = TRUE))[1];
  return(ret);
}

# This function calculates the number of year differences between the given date and current date
yearDiff <- function(x){
  return(as.numeric(year(Sys.Date()) - x))
}

# This function finds all numeric/integer features of a dataset
find_numeric_features <- function(myDataset){
  column_types <- sapply(names(myDataset), function(x) {class(myDataset[[x]])})
  numeric_columns <- names(column_types[column_types != "factor"])
  return(numeric_columns)
}

# This function creates various polynomial features
create_polynomial_feature <- function(myDataset){
  # Fetch the numeric/integer columns
  numeric_columns <- find_numeric_features(myDataset)
  
  for(columnName in numeric_columns){
    if(columnName != "SalePrice"){
      # Create 2nd degree features
      newColumnName <- paste(columnName, "sqr", sep = "_")
      myDataset[[newColumnName]] <- myDataset[[columnName]] ^ 2
      
      # Create 3rd degree featues
      newColumnName <- paste(columnName, "cub", sep = "_")
      myDataset[[newColumnName]] <- myDataset[[columnName]] ^ 3
      
      # Create square root featues
      newColumnName <- paste(columnName, "sqroot", sep = "_")
      myDataset[[newColumnName]] <- myDataset[[columnName]] ^ 0.5
      
      # Create cubroot featues
      newColumnName <- paste(columnName, "cubroot", sep = "_")
      myDataset[[newColumnName]] <- myDataset[[columnName]] ^ 0.33
      
      # Create inverse featues
      newColumnName <- paste(columnName, "inv", sep = "_")
      myDataset[[newColumnName]][myDataset[[columnName]] != 0] <- myDataset[[columnName]][myDataset[[columnName]] != 0] ^ (-1)
      myDataset[[newColumnName]][is.na(myDataset[[newColumnName]])] <- 0
      
      # Create inverse square features
      newColumnName <- paste(columnName, "sqinv", sep = "_")
      myDataset[[newColumnName]][myDataset[[columnName]] != 0] <- myDataset[[columnName]][myDataset[[columnName]] != 0] ^ (-2)
      myDataset[[newColumnName]][is.na(myDataset[[newColumnName]])] <- 0
    }
  }
  return(myDataset)
}

# This function standardizes all the numeric features
standardize_feature <- function(myDataset){
  # Fetch the numeric/integer columns
  numeric_columns <- find_numeric_features(myDataset)
  
  for(columnName in numeric_columns){
    if(columnName != "SalePrice"){
      columnMean <- mean(myDataset[[columnName]])
      columnSD <- sd(myDataset[[columnName]])
      myDataset[[columnName]] <- (myDataset[[columnName]] - columnMean)/columnSD
    }
  }
  return(myDataset)
}
```

# Data Reading and preparation

The dataset is offered in two separated fields, one for the training and another one for the test set. 

```{r Load Data}
original_training_data = read.csv(file = file.path("train.csv"))
original_test_data = read.csv(file = file.path("test.csv"))
```

To avoid applying the Feature Engineering process two times (once for training and once for test), you can just join both datasets (using the `rbind` function), apply your FE and then split the datasets again. However, if we try to do join the two dataframes as they are, we will get an error because they do not have the same columns: `test_data` does not have a column `SalePrice`. Therefore, we first create this column in the test set and then we join the data

```{r Joinning datasets}
# Create the column SalePrice in test set and assign the value to 0
original_test_data$SalePrice <- 0

# Create the column dataType in both train and test set and assign the value 'train' & 'test'
# This will help us to split the dataset from the dataType, not by position
original_training_data$dataType <- "train"
original_test_data$dataType <- "test"

dataset <- rbind(original_training_data, original_test_data)
```

Let's now visualize the dataset to see where to begin
```{r Dataset Visualization}
summary(dataset)
```

We can see some problems just by taking a look to the summary: the dataset has missing values, there are some categorical columns codified as numeric, it has different scales for the feature values. In addition, I will recommend you to take a deeper look to the data to detect more subtle issues: correlation between features, skewness in the feature values...

# Data Cleaning

The definition of "meaningless" depends on your data and your intuition. A feature can lack any importance because you know for sure that it does not going to have any impact in the final prediction (e.g., the ID of the house). In addition, there are features that could be relevant but present wrong, empty or incomplete values (this is typical when there has been a problem in the data gathering process). For example, the feature `Utilities` present a unique value, consequently it is not going to offer any advantage for prediction.

We remove meaningless features and incomplete cases.
```{r Feature ellimination}
dataset[c("Id")] <- NULL
```

## Hunting NAs

Our dataset is filled with missing values, therefore, before we can build any predictive model we'll clean our data by filling in all NA's with more appropriate values.
As another option, we could just remove the entries with null values (i.e., remove rows). However, in this situation (and in many other that you will face) this is out of the question: we have to provide a prediction for each and every one of the houses (required by the competition). 
Similarly, you could discard the features with null values (i.e., remove columns), but it would mean the removal of many features (and the information they provide).

As a rule of thumb, if you are allowed to discard some of your data and you do not have many null values (or you do not have a clear idea of how to impute them) you can safely delete them. If this is not the case, you must find a way to impute them (either by applying some knowledge of the addressed domain or by using some more advanced imputation method: https://topepo.github.io/caret/pre-processing.html#imputation)

Counting columns with null values.

```{r NAs discovery}
na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')

# Number of missing values in descending order
sort(colSums(sapply(dataset[na.cols], is.na)), decreasing = TRUE)

# Missing values in percentages
sort(sapply(dataset[na.cols], function(x){100*sum(is.na(x))/length(x)}), decreasing = TRUE);
```

How to clean up NAs, assign them default values, and assign features the correct type? You can write long pieces of code, use an external tool that will do most of the job for you (Dataiku?) or you can use the "Import Dataset" function in RStudio. Avoid using fancy external packages to import data, and rely on the most common ones ('pandas' in Python, or 'base' or 'readr' in R).

In any case, what we do here, is simply go through every single **factor** feature to: extend the number of possible levels to the new default for NAs (`None` or  `No` for categorical features or any other default value described in the documentation). For numerical values, we can just change the NA value for a default value, the median of the other values or some other value that you can infer (i.e., for the null values in the `GarageYrBlt` column you could use the year the house was built as a replacement value).

Similarly, we inpute values for numerical variables.
Apply this procedure for any column with null values in the dataset.

```{r Create level for NA}
# Create level
dataset$Alley = factor(dataset$Alley, levels=c(levels(dataset$Alley), "None"))
dataset$BsmtQual <- factor(dataset$BsmtQual, levels=c(levels(dataset$BsmtQual), "None"))
dataset$BsmtCond <- factor(dataset$BsmtCond, levels=c(levels(dataset$BsmtCond), "None"))
dataset$BsmtFinType1 <- factor(dataset$BsmtFinType1, levels=c(levels(dataset$BsmtFinType1), "None"))
dataset$BsmtFinType2 = factor(dataset$BsmtFinType2, levels=c(levels(dataset$BsmtFinType2), "None"))
dataset$Exterior1st <- factor(dataset$Exterior1st, levels=c(levels(dataset$Exterior1st), "Other"))
dataset$Fence <- factor(dataset$Fence, levels=c(levels(dataset$Fence), "None"))
dataset$FireplaceQu <- factor(dataset$FireplaceQu, levels=c(levels(dataset$FireplaceQu), "None"))
dataset$GarageType <- factor(dataset$GarageType, levels=c(levels(dataset$GarageType), "None"))
dataset$GarageCond <- factor(dataset$GarageCond, levels=c(levels(dataset$GarageCond), "None"))
dataset$GarageQual <- factor(dataset$GarageQual, levels=c(levels(dataset$GarageQual), "None"))
dataset$GarageFinish <- factor(dataset$GarageFinish, levels=c(levels(dataset$GarageFinish), "None"))
dataset$MSZoning <- factor(dataset$MSZoning, levels=c(levels(dataset$MSZoning), "OTH"))
dataset$MiscFeature <- factor(dataset$MiscFeature, levels=c(levels(dataset$MiscFeature), "None"))
dataset$PoolQC <- factor(dataset$PoolQC, levels=c(levels(dataset$PoolQC), "None"))

class(dataset)
```

and replacing the potential NA values in the dataset by that new 'factor' level.

```{r Impute value for NAs}
# Assume 0 linear feet of street connected to property if NA
dataset$LotFrontage[is.na(dataset$LotFrontage)] <- 0

# These factors contains NA values which actually mean the property has no such facility
dataset$MiscFeature[is.na(dataset$MiscFeature)] <- "None";
dataset$Fence[is.na(dataset$Fence)] <- "None";
dataset$PoolQC[is.na(dataset$PoolQC)] <- "None";
dataset$FireplaceQu[is.na(dataset$FireplaceQu)] <- "None";
dataset$Alley[is.na(dataset$Alley)] = "None"

# Unknown categorical feature set to a new category value - OTHER (more than 5 factors)
dataset$SaleType[is.na(dataset$SaleType)] <- "Oth";
dataset$MSZoning[is.na(dataset$MSZoning)] <- "OTH";
dataset$Exterior1st[is.na(dataset$Exterior1st)] <- "Other";
dataset$Exterior2nd[is.na(dataset$Exterior2nd)] <- "Other";

# Assume typical unless deductions are warranted
dataset$Functional[is.na(dataset$Functional)] = "Typ";

# Unknown categorical feature set to most common categorical value
dataset$Electrical[is.na(dataset$Electrical)] <- find_mode(dataset$Electrical);
dataset$KitchenQual[is.na(dataset$KitchenQual)] <- find_mode(dataset$KitchenQual);
dataset$Utilities[is.na(dataset$Utilities)] <- find_mode(dataset$Utilities);

# Special handling for consistency - MasVnrType and MasVnrArea (if Area is 0, Type should be None)
dataset[is.na(dataset$MasVnrType) & is.na(dataset$MasVnrArea),]$MasVnrArea <- 0
dataset[is.na(dataset$MasVnrType) & dataset$MasVnrArea >= 0,]$MasVnrType <- 'None'
dataset[dataset$MasVnrType %in% 'None' & dataset$MasVnrArea >= 0,]$MasVnrArea <- 0
dataset[!dataset$MasVnrType %in% 'None' & dataset$MasVnrArea == 0,]$MasVnrType <- 'None'

# Special handling for consistency - Garage (if Area is 0, the rest of the Garage feature should be None)
dataset$GarageType[is.na(dataset$GarageType)] <- "None";
dataset$GarageFinish[is.na(dataset$GarageFinish)] <- "None";
dataset$GarageQual[is.na(dataset$GarageQual)] <- "None";
dataset$GarageCond[is.na(dataset$GarageCond)] <- "None";
dataset$GarageArea[is.na(dataset$GarageArea)] <- 0;
dataset$GarageCars[is.na(dataset$GarageCars)] <- 0;

dataset[dataset$GarageArea == 0,]$GarageType <- 'None'

dataset[is.na(dataset$GarageYrBlt) & 
          dataset$GarageType %in% 'Detchd' & 
          dataset$GarageCars > 0 & 
          dataset$GarageArea > 0,]$GarageFinish <- find_mode(dataset$GarageFinish)

dataset[is.na(dataset$GarageYrBlt) & 
          dataset$GarageType %in% 'Detchd' & 
          dataset$GarageCars > 0 & 
          dataset$GarageArea > 0,]$GarageQual <- find_mode(dataset$GarageQual)

dataset[is.na(dataset$GarageYrBlt) & 
          dataset$GarageType %in% 'Detchd' & 
          dataset$GarageCars > 0 & 
          dataset$GarageArea > 0,]$GarageCond <- find_mode(dataset$GarageCond)
dataset$GarageYrBlt[is.na(dataset$GarageYrBlt)] <- dataset$YearBuilt[is.na(dataset$GarageYrBlt)];

# Special handling for consistency - Basement (if Area is 0, the rest of the Basement feature should be None)
dataset$BsmtQual[is.na(dataset$BsmtQual)] <- "None";
dataset$BsmtCond[is.na(dataset$BsmtCond)] <- "None";
dataset$BsmtFinType1[is.na(dataset$BsmtFinType1)] <- "None";
dataset$BsmtFinType2[is.na(dataset$BsmtFinType2)] <- "None";
dataset$BsmtExposure[is.na(dataset$BsmtExposure)] <- "No";

dataset[is.na(dataset$TotalBsmtSF),]$BsmtFinSF1 <- 0
dataset[is.na(dataset$TotalBsmtSF),]$BsmtFinSF2 <- 0
dataset[is.na(dataset$TotalBsmtSF),]$BsmtUnfSF <- 0
dataset[is.na(dataset$TotalBsmtSF) | (dataset$TotalBsmtSF == 0),]$BsmtFullBath <- 0
dataset[is.na(dataset$TotalBsmtSF) | (dataset$TotalBsmtSF == 0),]$BsmtHalfBath <- 0
dataset[is.na(dataset$TotalBsmtSF),]$TotalBsmtSF <- 0

# Check NA values
paste('There are', length(which(colSums(is.na(dataset)) > 0)), 'columns with missing values')
```

## Factorize features

If we go back to the summary of the dataset we can identify some numerical features that are actually categories: `MSSubClass` and the Year and Month in which the house was sold. What we have to do is to convert them to the proper 'class' or 'type' using the `as.factor` command.

```{r Factorize features}
# Convert Numerical to categorical
dataset$MoSold <- factor(dataset$MoSold)
dataset$YrSold <- factor(dataset$YrSold)
dataset$MSSubClass <- factor(dataset$MSSubClass)

# Convert dataType to categorical
dataset$dataType <- factor(dataset$dataType)
```

## Outliers
We will now focus on numerical values. If `NAs` where the natural enemy of categorical values, the main problem with numerical values are outliers (values which largely differ from the rest). Outliers can mislead the training of our models resulting in less accurate models and ultimately worse results.

In this section we seek to identify outliers to then properly deal with them. If we summarize the dataset, we can see variables which "Max." is much larger than the rest of values. These features are susceptible of containing outliers. Nevetheless, the easiest way to detect outliers is visualizing the numerical values; for instance, by `boxploting` the column values.

*Tip:* As explained in the feature engineering practice, the `boxplot` function can eliminate the outliers. However, if you apply it with the default values it is going to eliminate too much of them. You can adapt its working with the `outlier.size` param (https://ggplot2.tidyverse.org/reference/geom_boxplot.html), which I recommend you to set to at least 3. Another thing you can do is to compare the columns with outliers to the target variable (`SalePrice`) to visually check if there are some extreme values and just consider those as outliers.

```{r Outlier Detection}
# Split the dataset into train and test, so that we can remove outlier from training set
training_data <- dataset[dataset$dataType == "train",]
test <- dataset[dataset$dataType == "test",]

# Check for extreme outliers. Expecting a positive relationship with the target SalePrice
par(mfrow=c(2,5))
Sale_Price = "SalePrice"
for (col in colnames(training_data)) {
  if (!is.factor(training_data[[col]]) ){
    print(ggplot(training_data, aes_string(col, Sale_Price)) + 
            geom_point(color = 'blue') + 
            theme_bw() + 
            geom_smooth())
  }
}

# Remove observations that are outliers from the training set
training_data_no_outliers <- training_data[training_data$GrLivArea < 4000, ]
training_data_no_outliers <- training_data_no_outliers[training_data_no_outliers$LotArea < 100000, ]
training_data_no_outliers <- training_data_no_outliers[training_data_no_outliers$LotFrontage < 200, ]
training_data_no_outliers <- training_data_no_outliers[training_data_no_outliers$GarageArea < 1500, ]

# After removing outlier from training set, merge the train and test data.
dataset <- rbind(training_data_no_outliers, test)
```

# Feature Creation
This is the section to give free rein to your imagination and create all the features that might improve the final result. Do not worry if you add some "uninformative" feature because it will be removed by the later feature selection process.
Do not hesitate to consult the competition kernels (please cite anything you fork).

```{r Create new features}
# Total area
dataset$TotalArea <- dataset$LotFrontage + dataset$LotArea + dataset$MasVnrArea + 
  dataset$BsmtFinSF1 + dataset$BsmtFinSF2 + dataset$BsmtUnfSF + dataset$TotalBsmtSF + 
  dataset$X1stFlrSF + dataset$X2ndFlrSF + dataset$GrLivArea + dataset$GarageArea + 
  dataset$WoodDeckSF + dataset$OpenPorchSF + dataset$EnclosedPorch + dataset$X3SsnPorch + 
  dataset$ScreenPorch + dataset$LowQualFinSF + dataset$PoolArea

# Total area of 1st and 2nd floor
dataset$TotalArea1st2nd <- dataset$X1stFlrSF + dataset$X2ndFlrSF

# Years since house was built
dataset$HouseAge <- sapply(dataset$YearBuilt, yearDiff)

# Years since the house was last remodeling
dataset$AgeSinceLastRemod <- sapply(dataset$YearRemodAdd, yearDiff)

# Years since Garage is built
dataset$GarageAge <- abs(sapply(dataset$GarageYrBlt, yearDiff))

# Age of the house when sold
dataset$AgeWhenSold <- abs(as.numeric(dataset$YrSold) - dataset$YearBuilt)

# Total Num of Bathrooms
dataset$BsmtBath <- dataset$BsmtFullBath + dataset$BsmtHalfBath * 0.5
dataset$Bath <- dataset$FullBath + dataset$HalfBath * 0.5
dataset$TotalBath <- dataset$BsmtBath + dataset$Bath

# Total Porch area
dataset$TotalPorch <- dataset$OpenPorchSF + dataset$EnclosedPorch + 
  dataset$X3SsnPorch + dataset$ScreenPorch

# Combining the Month and Year.  Creating a timestamp feature for each transaction.
dataset$DtSold <- as.numeric(as.character(dataset$YrSold)) +
  as.numeric(as.character(dataset$MoSold))/12

# Grouping months (binary) with high sales transactions. This may have an impact to Sale Price.
analysedata <- dataset %>% group_by(MoSold) %>% summarise(Count = n())
analysedata$MoSold <- as.factor(analysedata$MoSold)
ggplot(data=analysedata, aes(x=MoSold, y=Count)) + 
  geom_bar(stat="identity", fill="steelblue") +
  theme_minimal()

dataset$PopularMonth <- factor(dataset$MoSold, 
                               levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), 
                               labels = c(0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0))

# Grouping types (binary) of dwellings with high transactions. This may have an impact to Sale Price.
analysedata <- dataset %>% group_by(MSSubClass) %>% summarise(Count = n())
analysedata$MSSubClass <- as.factor(analysedata$MSSubClass)
ggplot(data=analysedata, aes(x=MSSubClass, y=Count)) + 
  geom_bar(stat="identity", fill="steelblue") +
  theme_minimal()

dataset$PopularDwelling <- factor(dataset$MSSubClass, 
                                  levels = c(20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 
                                             120, 150, 160, 180, 190), 
                                  labels = c(1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 
                                             1, 0, 0, 0, 0))

## Group Neighborhood (numerical) based on mean Sale Price
analysedata <- dataset %>% group_by(Neighborhood) %>% summarise(Mean = mean(SalePrice))
analysedata$Neighborhood <- as.factor(analysedata$Neighborhood)
ggplot(data=analysedata, aes(x=Neighborhood, y=Mean)) +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()

dataset$NeighborhoodBin <- as.numeric(as.character(factor(
  dataset$Neighborhood, levels = c("MeadowV", "IDOTRR", "BrDale", "OldTown", "Edwards", "BrkSide", "Sawyer", "Blueste", "SWISU", "NAmes", "NPkVill", "Mitchel", "SawyerW", "Gilbert", "NWAmes", "Blmngtn", "CollgCr", "ClearCr", "Crawfor", "Veenker", "Somerst", "Timber", "StoneBr", "NoRidge", "NridgHt"),
  labels = c(0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4))))

# Convert Categorical features to numerical (ratings specially)
dataset$ExterQual <- as.numeric(as.character(factor(dataset$ExterQual,
                                                    levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                    labels = c(0, 1, 2, 3, 4, 5))))

dataset$ExterCond <- as.numeric(as.character(factor(dataset$ExterCond, 
                                                    levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                    labels = c(0, 1, 2, 3, 4, 5))))

dataset$BsmtQual <- as.numeric(as.character(factor(dataset$BsmtQual, 
                                                   levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                   labels = c(0, 1, 2, 3, 4, 5))))

dataset$BsmtCond <- as.numeric(as.character(factor(dataset$BsmtCond, 
                                                   levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                   labels = c(0, 1, 2, 3, 4, 5))))

dataset$HeatingQC <- as.numeric(as.character(factor(dataset$HeatingQC, 
                                                    levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                    labels = c(0, 1, 2, 3, 4, 5))))

dataset$KitchenQual <- as.numeric(as.character(factor(dataset$KitchenQual, 
                                                      levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"),
                                                      labels = c(0, 1, 2, 3, 4, 5))))

dataset$FireplaceQu <- as.numeric(as.character(factor(dataset$FireplaceQu, 
                                                      levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"),
                                                      labels = c(0, 1, 2, 3, 4, 5))))

dataset$GarageQual <- as.numeric(as.character(factor(dataset$GarageQual, 
                                                     levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                     labels = c(0, 1, 2, 3, 4, 5))))

dataset$GarageCond <- as.numeric(as.character(factor(dataset$GarageCond, 
                                                     levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                     labels = c(0, 1, 2, 3, 4, 5))))

dataset$PoolQC <- as.numeric(as.character(factor(dataset$PoolQC, 
                                                 levels = c("None", "Po", "Fa", "TA", "Gd", "Ex"), 
                                                 labels = c(0, 1, 2, 3, 4, 5))))

dataset$BsmtExposure <- as.numeric(as.character(factor(dataset$BsmtExposure, 
                                                       levels = c("None", "No", "Mn", "Av", "Gd"), 
                                                       labels = c(0, 1, 2, 3, 4))))

dataset$BsmtFinType1 <- as.numeric(as.character(factor(dataset$BsmtFinType1, 
                                                       levels = c("None", "Unf", "LwQ", "Rec", "BLQ",
                                                                  "ALQ","GLQ"), 
                                                       labels = c(0, 1, 2, 3, 4, 5, 6))))

dataset$BsmtFinType2 <- as.numeric(as.character(factor(dataset$BsmtFinType2, 
                                                       levels = c("None", "Unf", "LwQ", "Rec", "BLQ", 
                                                                  "ALQ", "GLQ"), 
                                                       labels = c(0, 1, 2, 3, 4, 5, 6))))

dataset$Functional <- as.numeric(as.character(factor(dataset$Functional, 
                                                     levels = c("Sal", "Sev", "Maj2", "Maj1", "Mod", 
                                                                "Min2", "Min1", "Typ"), 
                                                     labels = c(1, 2, 3, 4, 5, 6, 7, 8))))

dataset$GarageFinish <- as.numeric(as.character(factor(dataset$GarageFinish, 
                                                       levels = c("None", "Unf", "RFn", "Fin"), 
                                                       labels = c(0, 1, 2, 3))))

dataset$Fence <- as.numeric(as.character(factor(dataset$Fence, 
                                                levels = c("None", "MnWw", "GdWo", "MnPrv", "GdPrv"), 
                                                labels = c(0, 1, 2, 3, 4))))

# Changing scoring system (Numerical) of the overall condition and quality of the property
dataset$OverallQual2 <- as.numeric(as.character(factor(dataset$OverallQual, 
                                                       levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 
                                                       labels = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3))))

dataset$OverallCond2 <- as.numeric(as.character(factor(dataset$OverallCond, 
                                                       levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 
                                                       labels = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3))))

# Is there a reason the price is a discounted price (binary)?
dataset$SaleNormal <- factor(dataset$SaleCondition, 
                             levels = c("Normal", "Abnorml", "AdjLand", 
                                        "Alloca", "Family", "Partial"),
                             labels = c(1, 0, 0, 0, 0, 1))

# Is it an uncompleted sale (binary)?
dataset$SaleComplete <- factor(dataset$SaleCondition, 
                               levels = c("Normal", "Abnorml", "AdjLand", 
                                          "Alloca", "Family", "Partial"),
                               labels = c(0, 0, 0, 0, 0, 1))

# Is the shape regular (binary)?
dataset$IsRegularShape <- factor(dataset$LotShape, 
                                 levels = c("Reg", "IR1", "IR2", "IR3"),
                                 labels = c(0, 0, 0, 1))

# Is Prime Zone (binary)?
dataset$IsPrimeZone <- factor(dataset$MSZoning, 
                              levels = c("A", "C (all)", "FV", "I", "RH", "RL", "RP", "RM", "OTH"),
                              labels = c(0, 0, 1, 0, 0, 1, 1, 0, 0))

# Is Normal Zone (binary)?
dataset$IsNormalZone <- factor(dataset$MSZoning, 
                               levels = c("A", "C (all)", "FV", "I", "RH", "RL", "RP", "RM", "OTH"),
                               labels = c(0, 0, 0, 0, 1, 0, 0, 1, 0))

# Is there Alley access?
dataset$IsAlleyAccess <- factor(dataset$Alley, 
                                levels = c("None", "Grvl", "Pave"),
                                labels = c(0, 1, 1))

# Is Basement present
dataset$IsBasementPresent <- factor(dataset$BsmtCond, 
                                    levels = c(0, 1, 2, 3, 4, 5),
                                    labels = c(0, 1, 1, 1, 1, 1))

# Is Garage present
dataset$IsGaragePresent <- factor(dataset$GarageCond, 
                                  levels = c(0, 1, 2, 3, 4, 5),
                                  labels = c(0, 1, 1, 1, 1, 1))

# Is Pool present
dataset$IsPoolPresent <- factor(dataset$PoolQC, 
                                levels = c(0, 1, 2, 3, 4, 5),
                                labels = c(0, 1, 1, 1, 1, 1))

# Is Fence present
dataset$IsFencePresent <- factor(dataset$Fence, 
                                 levels = c(0, 1, 2, 3, 4, 5),
                                 labels = c(0, 1, 1, 1, 1, 1))

# Is Fireplace present
dataset$IsFireplacePresent <- factor(dataset$FireplaceQu, 
                                     levels = c(0, 1, 2, 3, 4, 5),
                                     labels = c(0, 1, 1, 1, 1, 1))

# Create polynomial features of numeric variables
dataset <- create_polynomial_feature(dataset)

# Check NA values
paste('There are', length(which(colSums(is.na(dataset)) > 0)), 'columns with missing values')
```

## Feature Ellimination

Let's remove unncessary features from the data. We will remove feature when
1. Features are no more needed after creation of new features.
2. Features that have no corrlation with the target.

```{r Remove features}
# As I have choosen Lasso, so manual feature removal is no more needed, as Lasso will do that.
```

# Skewness

We now need to detect skewness in the Target value. Let's see what is the effect of skewness on a variable, and plot it using ggplot. The way of getting rid of the skewness is to use the `log` (or the `log1p`) of the values of that feature, to flatten it. To reduce right skewness, take roots or logarithms or reciprocals (x to 1/x). This is the commonest problem in practice. To reduce left skewness, take squares or cubes or higher powers.

While building predictive models we often see skewness in the target variable. Then we generally take transformations to make it more normal. We generally do it for linear models and not for tree based models. This actually means that our distribution is not normal, we are deliberately making it normal for prediction.

```{r}
df <- rbind(data.frame(version="price",x=original_training_data$SalePrice),
            data.frame(version="log(price+1)",x=log(original_training_data$SalePrice + 1)))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```

We therefore transform the target value applying log
```{r Log transform the target for official scoring}
# Log transform the target for official scoring
dataset$SalePrice <- log1p(dataset$SalePrice)
```


The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform log transformation

I will set up my threshold for the skewness in 0.75. I place that value in that variable to adjust its value in a single place, in case I have to perform multiple tests.

```{r}
skewness_threshold = 0.75
```

Now, let's compute the skewness of each feature that is not 'factor' nor 'character'. So, I'm only interested in continuous values. One possible way of doing it is the following: First, lets determine what is the 'class' or data type of each of my features.

To do so, instead of `loops`, we will use the `apply` family of functions. They will __apply__ a method to each **row** or **column** of your dataset. It will depend on what to do specify as the first argument of the method. 

```
sapply(list_of_elements, function)
```

What we want to determine is the class of each column or feature, and to do so, we use the `class` method from R. We will pass the actual column or feature from our dataset (dataframe):

```
class(dataframe_name[['column_name']])
```

Both ideas together produce a nice code chunk like the following:
```{r}
# Fetch the numeric/integer columns
numeric_columns <- find_numeric_features(dataset)
```

And now, with that information, we need to calculate the skewness of each column whose name is our list of __factor__ (or categorical) features. We use the `sapply` method again, to compute the skewness of each column whose name is in the list of `numeric_columns`.
```{r}
# skew of each variable
skew <- sapply(numeric_columns, function(x) {e1071::skewness(dataset[[x]], na.rm = T)})
```


What we do need to make now is to apply the log to those whose skewness value is above a given threshold that we've set in 0.75. We should test different hypothesis with our threshold too.
```{r}
# transform all variables above a threshold skewness.
skew <- skew[abs(skew) > skewness_threshold]
for(x in names(skew)) {
  if(!is.na(x)){
    dataset[[x]] <- log(dataset[[x]] + 1)
  }
}
```

# Scaling

Let's make all the numeric features scale-less by applying standardization.

```{r scaling}
# Standardize all the numeric features using mean and standard deviation
dataset <- standardize_feature(dataset)
```


# Train, Validation Spliting

To facilitate the data cleaning and feature engineering we merged train and test datasets. We now split them again to create our final model.

```{r Train test split}
training_data <- dataset[dataset$dataType == "train",]
test <- dataset[dataset$dataType == "test",]

```

We are going to split the annotated dataset in training and validation for the later evaluation of our regression models
```{r Train Validation split}
splits <- splitdf(training_data, seed=1)
training <- splits$trainset
validation <- splits$testset
```

# Feature Selection
We here start the Feature Selection.

## Embedded

Finally, we will experiment with embedded methods.

### Ridge/Lasso Regression

For this exercise, we are going to make use of the <a href="https://cran.r-project.org/web/packages/glmnet/index.html">`glmnet`</a> library. Take a look to the library to fit a glmnet model for Ridge/Lasso Regression, using a grid of lambda values.

The only think that changes between Lasso and Ridge is the `alpha` parameter. The remaining part of the exercise is equivalent.

```{r Ridge/Lasso Regression, warning=FALSE}
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.lasso.mod <- train(SalePrice ~ ., data = training, 
                         method = "glmnet", 
                         metric = "RMSE",
                         trControl=train_control_config,
                         tuneGrid = expand.grid(alpha = 1, lambda = lambdas))
```

The parameter `alpha = 0` means that we want to use the Ridge Regression way of expressing the penalty in regularization. If you replace that by `alpha = 1` then you get Lasso.

#### Evaluation

Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r Ridge/Lasso RMSE}
plot(ridge.lasso.mod)
```

Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. However, as we can see at the top of the features, there is no feature selection; i.e., the model always consider the 225 parameters.

```{r Ridge/Lasso Coefficients}
plot(ridge.lasso.mod$finalModel)
```

```{r Ridge/Lasso Evaluation}
ridge.lasso.mod.pred <- predict(ridge.lasso.mod, validation)
ridge.lasso.mod.pred[is.na(ridge.lasso.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted = (exp(ridge.lasso.mod.pred) -1), observed = (exp(validation$SalePrice) -1)))

ridge.lasso.mod.rmse <- sqrt(mean((ridge.lasso.mod.pred - validation$SalePrice)^2))
ridge.lasso.mod.price_error <- mean(abs((exp(ridge.lasso.mod.pred) -1) - (exp(validation$SalePrice) -1)))

ggplot(my_data, aes(predicted, observed)) +
  geom_point() + geom_smooth(method = "glm") +
  labs(x="Predicted") +
  ggtitle(ggtitle(paste("Ridge", 'RMSE: ', format(round(ridge.lasso.mod.rmse, 4), nsmall=4), ' --> Price ERROR:', format(round(ridge.lasso.mod.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
  scale_x_continuous(labels = scales::comma) + 
  scale_y_continuous(labels = scales::comma)

```


Rank the variables according to the importance attributed by the model.
```{r}
# Print, plot variable importance
plot(varImp(ridge.lasso.mod), top = 20) # 20 most important features

ridge.lasso.VarImp <- varImp(ridge.lasso.mod,scale=F)
ridge.lasso.var.importance <- ridge.lasso.VarImp$importance
varsSelected <- length(which(ridge.lasso.var.importance$Overall!=0))
varsNotSelected <- length(which(ridge.lasso.var.importance$Overall==0))
cat('Lasso uses', varsSelected, 'variables in its model, and did not select', varsNotSelected, 'variables.')

```

# Final Submission

Based on your analysis, you have to decide which cleaning and feature engineering procedures make sense in order to create your final model.
We splitted the original training data into train and validation to evaluate the candidate models. In order to generate the final submission we have to take instead all the data at our disposal.
In addition, remember that we also applied a log transformation to the target variable, to revert this transformation you have to use the exp function.

Let's see this with the code. Imagine that your final model is the `ridge.mod` that we have just created. In order to generate the final submission:

```{r Final Submission}
# Train the model using all the data
final.model <- ridge.lasso.mod

# Predict the prices for the test data (i.e., we use the exp function to revert the log transformation that we applied to the target variable)
final.pred <- as.numeric(exp(predict(final.model, test))-1) 
final.pred[is.na(final.pred)]
hist(final.pred, main="Histogram of Predictions", xlab = "Predictions")

lasso_submission <- data.frame(Id = original_test_data$Id, SalePrice= (final.pred))
colnames(lasso_submission) <-c("Id", "SalePrice")
write.csv(lasso_submission, file = "submission_6.csv", row.names = FALSE) 

```